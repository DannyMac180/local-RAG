{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import ObsidianLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "langchain_api_key = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load, split and store embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = ObsidianLoader(\"/Users/danielmcateer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Ideaverse\")\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve and generate using the relevant notes from Obsidian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "llm = ChatAnthropic(temperature=0.5, anthropic_api_key=anthropic_api_key, model_name=\"claude-3-opus-20240229\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run RAG chain using LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain_from_docs = (\n",
    "    RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain_with_source = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ").assign(answer=rag_chain_from_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get a response from the RAG chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "question: What does Andrej Karpathy say about the Transformers architecture?\n",
      "\n",
      "context: [Document(page_content=\"that are pretty obvious that they don't do well to going back to the original you know tail the cognitive tape right you can look at all The places the human is clearly superior to the transformer and start to look out for architectures that might change that dynamic. You know if you said how many meaningful breakthroughs are we away from the AI scientists that can produce your reka moments at a pace faster than human scientists tend to. It doesn't feel like it's that many you know I would say probably more than zero but it's probably less than four. ([Time\\xa01:45:00](https://share.snipd.com/snip/ed3e95a6-2600-4f78-abd6-e4225b3340d2))\", metadata={'created': 1709001561.6959817, 'last_accessed': 1709158969.422937, 'last_modified': 1709001554.7333877, 'path': '/Users/danielmcateer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Ideaverse/Readwise/Podcasts/Looking Up the AI Exponential With Azeem Azhar of the Exponential View.md', 'source': 'Looking Up the AI Exponential With Azeem Azhar of the Exponential View.md', 'tags': 'podcasts'}), Document(page_content=\"that are pretty obvious that they don't do well to going back to the original you know tail the cognitive tape right you can look at all The places the human is clearly superior to the transformer and start to look out for architectures that might change that dynamic. You know if you said how many meaningful breakthroughs are we away from the AI scientists that can produce your reka moments at a pace faster than human scientists tend to. It doesn't feel like it's that many you know I would say probably more than zero but it's probably less than four. ([Time\\xa01:45:00](https://share.snipd.com/snip/ed3e95a6-2600-4f78-abd6-e4225b3340d2))\", metadata={'created': 1709580801.0743194, 'last_accessed': 1709580802.2102733, 'last_modified': 1709580409.0, 'path': '/Users/danielmcateer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Ideaverse/ideaverse/Readwise/Podcasts/Looking Up the AI Exponential With Azeem Azhar of the Exponential View.md', 'source': 'Looking Up the AI Exponential With Azeem Azhar of the Exponential View.md', 'tags': 'podcasts'}), Document(page_content='Wealth is generating the ability to transform the world in the way that benefits you and others\\nArt as your unique natural expression\\n<<<<<<< HEAD\\n=======\\nCognitive Revolution Mamba episode summary notes\\nKey points\\n1. Mamba can make up for the Transformers weakness when it comes to “long term memory” via the state representation\\n2. Hardware aware algorithms are really the magic sauce and what make it efficient enough to work. That’s the knowledge contribution from an engineering perspective\\n3. The longer the sequence, the better it works\\n>>>>>>> origin/main\\n---END #17---\\n---BEGIN #18---\\nZettelkasten > Fleeting Notes > Humanity has evolved an Informational layer:\\nCreated time: 2023-07-08T18:46', metadata={'created': 1709090687.4137013, 'last_accessed': 1709601912.3933043, 'last_modified': 1707359910.69821, 'path': '/Users/danielmcateer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Ideaverse/Zettelkasten/Permanent Notes/Worldview 2.md', 'source': 'Worldview 2.md'}), Document(page_content='Wealth is generating the ability to transform the world in the way that benefits you and others\\nArt as your unique natural expression\\n<<<<<<< HEAD\\n=======\\nCognitive Revolution Mamba episode summary notes\\nKey points\\n1. Mamba can make up for the Transformers weakness when it comes to “long term memory” via the state representation\\n2. Hardware aware algorithms are really the magic sauce and what make it efficient enough to work. That’s the knowledge contribution from an engineering perspective\\n3. The longer the sequence, the better it works\\n>>>>>>> origin/main\\n---END #17---\\n---BEGIN #18---\\nZettelkasten > Fleeting Notes > Humanity has evolved an Informational layer:\\nCreated time: 2023-07-08T18:46', metadata={'created': 1709581247.441547, 'last_accessed': 1709583844.790024, 'last_modified': 1709580409.0, 'path': '/Users/danielmcateer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Ideaverse/ideaverse/Zettelkasten/Permanent Notes/Worldview 2.md', 'source': 'Worldview 2.md'})]\n",
      "\n",
      "answer: According to the given context, Andrej Karpathy suggests that there are obvious areas where humans are clearly superior to the transformer architecture. He believes that we are probably more than zero but less than four meaningful breakthroughs away from AI scientists that can produce Eureka moments at a pace faster than human scientists.{'question': 'What does Andrej Karpathy say about the Transformers architecture?', 'context': [Document(page_content=\"that are pretty obvious that they don't do well to going back to the original you know tail the cognitive tape right you can look at all The places the human is clearly superior to the transformer and start to look out for architectures that might change that dynamic. You know if you said how many meaningful breakthroughs are we away from the AI scientists that can produce your reka moments at a pace faster than human scientists tend to. It doesn't feel like it's that many you know I would say probably more than zero but it's probably less than four. ([Time\\xa01:45:00](https://share.snipd.com/snip/ed3e95a6-2600-4f78-abd6-e4225b3340d2))\", metadata={'created': 1709001561.6959817, 'last_accessed': 1709158969.422937, 'last_modified': 1709001554.7333877, 'path': '/Users/danielmcateer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Ideaverse/Readwise/Podcasts/Looking Up the AI Exponential With Azeem Azhar of the Exponential View.md', 'source': 'Looking Up the AI Exponential With Azeem Azhar of the Exponential View.md', 'tags': 'podcasts'}), Document(page_content=\"that are pretty obvious that they don't do well to going back to the original you know tail the cognitive tape right you can look at all The places the human is clearly superior to the transformer and start to look out for architectures that might change that dynamic. You know if you said how many meaningful breakthroughs are we away from the AI scientists that can produce your reka moments at a pace faster than human scientists tend to. It doesn't feel like it's that many you know I would say probably more than zero but it's probably less than four. ([Time\\xa01:45:00](https://share.snipd.com/snip/ed3e95a6-2600-4f78-abd6-e4225b3340d2))\", metadata={'created': 1709580801.0743194, 'last_accessed': 1709580802.2102733, 'last_modified': 1709580409.0, 'path': '/Users/danielmcateer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Ideaverse/ideaverse/Readwise/Podcasts/Looking Up the AI Exponential With Azeem Azhar of the Exponential View.md', 'source': 'Looking Up the AI Exponential With Azeem Azhar of the Exponential View.md', 'tags': 'podcasts'}), Document(page_content='Wealth is generating the ability to transform the world in the way that benefits you and others\\nArt as your unique natural expression\\n<<<<<<< HEAD\\n=======\\nCognitive Revolution Mamba episode summary notes\\nKey points\\n1. Mamba can make up for the Transformers weakness when it comes to “long term memory” via the state representation\\n2. Hardware aware algorithms are really the magic sauce and what make it efficient enough to work. That’s the knowledge contribution from an engineering perspective\\n3. The longer the sequence, the better it works\\n>>>>>>> origin/main\\n---END #17---\\n---BEGIN #18---\\nZettelkasten > Fleeting Notes > Humanity has evolved an Informational layer:\\nCreated time: 2023-07-08T18:46', metadata={'created': 1709090687.4137013, 'last_accessed': 1709601912.3933043, 'last_modified': 1707359910.69821, 'path': '/Users/danielmcateer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Ideaverse/Zettelkasten/Permanent Notes/Worldview 2.md', 'source': 'Worldview 2.md'}), Document(page_content='Wealth is generating the ability to transform the world in the way that benefits you and others\\nArt as your unique natural expression\\n<<<<<<< HEAD\\n=======\\nCognitive Revolution Mamba episode summary notes\\nKey points\\n1. Mamba can make up for the Transformers weakness when it comes to “long term memory” via the state representation\\n2. Hardware aware algorithms are really the magic sauce and what make it efficient enough to work. That’s the knowledge contribution from an engineering perspective\\n3. The longer the sequence, the better it works\\n>>>>>>> origin/main\\n---END #17---\\n---BEGIN #18---\\nZettelkasten > Fleeting Notes > Humanity has evolved an Informational layer:\\nCreated time: 2023-07-08T18:46', metadata={'created': 1709581247.441547, 'last_accessed': 1709583844.790024, 'last_modified': 1709580409.0, 'path': '/Users/danielmcateer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Ideaverse/ideaverse/Zettelkasten/Permanent Notes/Worldview 2.md', 'source': 'Worldview 2.md'})], 'answer': 'According to the given context, Andrej Karpathy suggests that there are obvious areas where humans are clearly superior to the transformer architecture. He believes that we are probably more than zero but less than four meaningful breakthroughs away from AI scientists that can produce Eureka moments at a pace faster than human scientists.'}\n"
     ]
    }
   ],
   "source": [
    "output = {}\n",
    "curr_key = None\n",
    "for chunk in rag_chain_with_source.stream(\"What does Andrej Karpathy say about the Transformers architecture?\"):\n",
    "    for key in chunk:\n",
    "        if key not in output:\n",
    "            output[key] = chunk[key]\n",
    "        else:\n",
    "            output[key] += chunk[key]\n",
    "        if key != curr_key:\n",
    "            print(f\"\\n\\n{key}: {chunk[key]}\", end=\"\", flush=True)\n",
    "        else:\n",
    "            print(chunk[key], end=\"\", flush=True)\n",
    "        curr_key = key\n",
    "print(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore.delete_collection()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
